{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages needed for processing\n",
    "import re\n",
    "import json\n",
    "import xml\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from TwitterAPI import TwitterAPI # in case you need to install this package, see practical 6\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import requests\n",
    "\n",
    "# disabling urllib3 warnings\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#If you need add any additional packages, then add them below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the list of three keywords you selected to the variables keywords below\n",
    "#e.g. keywords = [\"abc\", \"def\", \"ghi\"]\n",
    "\n",
    "keywords =  [\"rain\", \"LPL\", \"gas price\"]\n",
    "\n",
    "group_id = \"OVA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"\" #API key\n",
    "CONSUMER_SECRET = \"\" #API Key Secret\n",
    "OAUTH_TOKEN = \"\"\n",
    "OAUTH_TOKEN_SECRET = \"\"\n",
    "\n",
    "# Authenticating with your application credentials\n",
    "api = TwitterAPI(CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET, api_version='2')\n",
    "\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo coordinations of the desired place\n",
    "\n",
    "PLACE_LAT = 7.8731\n",
    "PLACE_LON = 80.7718\n",
    "DELTA_LAT = 1.0\n",
    "DELTA_LON = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tweets(api, keyword, batch_count, total_count, latitude, longitude, delta_lat, delta_lon):\n",
    "\n",
    "    # the collection of tweets to be returned\n",
    "    tweets_unfiltered = []\n",
    "    tweets = []\n",
    "    total_tweets_pulled = 0; \n",
    "    \n",
    "    # the number of tweets within a single query\n",
    "    batch_count = str(batch_count)\n",
    "      \n",
    "    resp = api.request('tweets/search/recent', \n",
    "                                    {'query': keyword,\n",
    "                                        'max_results': batch_count,                                     \n",
    "                                        'tweet.fields': {'lang':'en'},      \n",
    "                                        'place.fields':{\n",
    "                                        'geo': {\n",
    "                                        \"type\": \"Feature\",\n",
    "                                        \"bbox\": [\n",
    "                                        longitude - delta_lon,\n",
    "                                        latitude - delta_lat,\n",
    "                                        longitude + delta_lon,\n",
    "                                        latitude + delta_lat\n",
    "                                        ],\n",
    "                                        \"properties\": {}\n",
    "                                        }}})\n",
    "    \n",
    "    \n",
    "    #if the resonse had an error\n",
    "    if ('errors' in resp.json()):\n",
    "        errors = resp.json()['title']\n",
    "        if (errors == 'Invalid Request'):\n",
    "            print('Too many attempts to load tweets or too many tweets to load.')\n",
    "            print('You need to wait for a few minutes before accessing Twitter API again or reduce max_results.')\n",
    "    \n",
    "    else:\n",
    "        tweets_unfiltered += resp\n",
    "        \n",
    "        ids = [int(tweet['id']) for tweet in tweets_unfiltered]\n",
    "        max_id_str = str(min(ids))\n",
    "        \n",
    "        tweets = [tweet for tweet in tweets_unfiltered if (('RT @' not in tweet['text']) & (tweet['lang'] == 'en'))]\n",
    "        total_tweets_pulled\n",
    "        \n",
    "        # loop until as many tweets as total_count is collected\n",
    "        number_of_tweets = len(tweets)\n",
    "        \n",
    "        while number_of_tweets < total_count:\n",
    "\n",
    "        \n",
    "            resp = api.request('tweets/search/recent', \n",
    "                                            {'query': keyword,\n",
    "                                            'max_results': batch_count,                                    \n",
    "                                            'until_id': max_id_str,\n",
    "                                            'tweet.fields': {'lang':'en'},      \n",
    "                                            'place.fields':{\n",
    "                                            'geo': {\n",
    "                                            \"type\": \"Feature\",\n",
    "                                            \"bbox\": [\n",
    "                                            longitude - delta_lon,\n",
    "                                            latitude - delta_lat,\n",
    "                                            longitude + delta_lon,\n",
    "                                            latitude + delta_lat\n",
    "                                            ],\n",
    "                                            \"properties\": {}\n",
    "                                            }}})\n",
    "\n",
    "                \n",
    "            tweets_unfiltered += resp\n",
    "            tweets = [tweet for tweet in tweets_unfiltered if (('RT @' not in tweet['text']) & (tweet['lang'] == 'en'))]\n",
    "    \n",
    "            ids = [int(tweet['id']) for tweet in tweets_unfiltered]\n",
    "            max_id_str = str(min(ids))\n",
    "                \n",
    "            number_of_tweets = len(tweets)\n",
    "            \n",
    "            print(\"{} tweets are collected for keyword {}. \".format(number_of_tweets, keyword))\n",
    "            \n",
    "        print(\"{} total tweets pulled. \".format(len(tweets_unfiltered)))\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the tweets for three assigned keywords, \n",
    "# Your function call should look like this:  \n",
    "#      retrieve_tweets(api, keyword, batch_count, total_count, latitude, longitude, delta_lat, delta_lon)\n",
    "\n",
    "k1_tweets = retrieve_tweets(api, keywords[0], 50, 200, PLACE_LAT, PLACE_LON, DELTA_LAT, DELTA_LON)\n",
    "k2_tweets = retrieve_tweets(api, keywords[1], 50, 200, PLACE_LAT, PLACE_LON, DELTA_LAT, DELTA_LON)\n",
    "k3_tweets = retrieve_tweets(api, keywords[2], 50, 200, PLACE_LAT, PLACE_LON, DELTA_LAT, DELTA_LON)\n",
    "\n",
    "# PLEASE NOTE THAT IF YOU RUN THIS CELL, IT MIGHT TAKE A WHILE TO DOWNLOAD ALL THE TWEETS REQUIRED.\n",
    "# MAKE SURE THAT YOU WAIT UNTILL THE CELL FINISHES RUNNING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of {} Tweets for the Keyword {}.\".format(len(k1_tweets), keywords[0]))\n",
    "print(\"Total of {} Tweets for the Keyword {}.\".format(len(k2_tweets), keywords[1]))\n",
    "print(\"Total of {} Tweets for the Keyword {}.\".format(len(k3_tweets), keywords[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type of tweets\n",
    "print(type(k1_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k1_tweets[0].keys())\n",
    "\n",
    "'''\n",
    "Code to print out the text of the first  tweet collected for each keyword.\n",
    "'''\n",
    "\n",
    "print(\"\\nThe text of the first tweet for \\\"{}\\\":\\n\".format(keywords[0]))\n",
    "\n",
    "print(k1_tweets[0][\"text\"])\n",
    "\n",
    "\n",
    "print(\"\\nThe text of the first tweet for \\\"{}\\\":\\n\".format(keywords[1]))\n",
    "\n",
    "print(k2_tweets[0][\"text\"])\n",
    "\n",
    "\n",
    "print('\\nThe text of the first tweet for \\\"{}\\\":\\n'.format(keywords[2]))\n",
    "\n",
    "print(k3_tweets[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(obj, filename):\n",
    "    \"\"\"\n",
    "    saves a list of dictionaries into a json file\n",
    "    \n",
    "    obj: list of dictionaries\n",
    "    filename: filename\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(obj, fp, indent=4, sort_keys=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tweets in three json files, one for each keyword\n",
    "\n",
    "save_to_json(k1_tweets, \"{}_{}.json\".format(group_id, keywords[0].replace(\" \", \"_\")))\n",
    "save_to_json(k2_tweets, \"{}_{}.json\".format(group_id, keywords[1].replace(\" \", \"_\")))\n",
    "save_to_json(k3_tweets, \"{}_{}.json\".format(group_id, keywords[2].replace(\" \", \"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(filename):\n",
    "    \"\"\"\n",
    "    reads from a json file and saves the result in a list named data\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fp:\n",
    "        content = fp.read()\n",
    "\n",
    "    \n",
    "    data = json.loads(content)\n",
    "    return data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Three function calls to load data from three json files you have saved from Part 1.\n",
    "'''\n",
    "\n",
    "k1_tweets = read_json_file(\"{}_{}.json\".format(group_id, keywords[0].replace(\" \", \"_\")))\n",
    "k2_tweets = read_json_file(\"{}_{}.json\".format(group_id, keywords[1].replace(\" \", \"_\")))\n",
    "k3_tweets = read_json_file(\"{}_{}.json\".format(group_id, keywords[2].replace(\" \", \"_\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to print out the number of tweets containes in three variables: \n",
    "k1_tweets, k2_tweets and k3_tweets\n",
    "'''\n",
    "\n",
    "\n",
    "print(\"Total of {} Tweets for the Keyword {}.\".format(len(k1_tweets), keywords[0]))\n",
    "print(\"Total of {} Tweets for the Keyword {}.\".format(len(k2_tweets), keywords[1]))\n",
    "print(\"Total of {} Tweets for the Keyword {}.\".format(len(k3_tweets), keywords[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_short_tweet(tweet):\n",
    "    '''\n",
    "    Check if the text of \"tweet\" has less than 50 characters\n",
    "    '''    \n",
    "    if(len(tweet[\"text\"]) < 50):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Codes to remove all tweets which have less than 50 characters in variables \n",
    "k1_tweets, k2_tweets and k3_tweets and store the results in the new variables \n",
    "k1_tweets_filtered, k2_tweets_filtered and k3_tweets_filtered respectively\n",
    "'''\n",
    "\n",
    "k1_tweets_filtered = [tweet for tweet in k1_tweets if not is_short_tweet(tweet)]\n",
    "k2_tweets_filtered = [tweet for tweet in k2_tweets if not is_short_tweet(tweet)]\n",
    "k3_tweets_filtered = [tweet for tweet in k3_tweets if not is_short_tweet(tweet)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# these lines below print the number of tweets for each keyword before and after filtered.\n",
    "print(len(k1_tweets), len(k1_tweets_filtered))\n",
    "print(len(k2_tweets), len(k2_tweets_filtered))\n",
    "print(len(k3_tweets), len(k3_tweets_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each keyword, print out the number of tweets that have been removed.\n",
    "'''\n",
    "\n",
    "print(\"{} from keyword {} removed.\".format(len(k1_tweets) - len(k1_tweets_filtered), keywords[0]))\n",
    "print(\"{} from keyword {} removed.\".format(len(k2_tweets) - len(k2_tweets_filtered), keywords[1]))\n",
    "print(\"{} from keyword {} removed.\".format(len(k3_tweets) - len(k3_tweets_filtered), keywords[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to print out the first 5 tweets for each keyword.\n",
    "'''\n",
    "\n",
    "print('The first 5 tweets for \\\"{}\\\":\\n'.format(keywords[0]))\n",
    "\n",
    "for i in range(5):print(k1_tweets_filtered[i])\n",
    "\n",
    "print('\\nThe first 5 tweets for \\\"{}\\\":\\n'.format(keywords[1]))\n",
    "\n",
    "for i in range(5):print(k2_tweets_filtered[i])\n",
    "\n",
    "\n",
    "print('\\nThe first 5 tweets for \\\"{}\\\":\\n'.format(keywords[2]))\n",
    "\n",
    "for i in range(5):print(k3_tweets_filtered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "def pre_process(doc):\n",
    "    \"\"\"\n",
    "    pre-processes a doc\n",
    "      * Converts the tweet into lower case,\n",
    "      * removes the URLs,\n",
    "      * removes the punctuations\n",
    "      * tokenizes the tweet\n",
    "      * removes words less that 3 characters\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    # getting rid of non ascii codes\n",
    "    doc = remove_non_ascii(doc)\n",
    "    \n",
    "    # replacing URLs\n",
    "    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n",
    "    doc = re.sub(url_pattern, 'url', doc) \n",
    "\n",
    "    # removing dollars and usernames and other unnecessary stuff\n",
    "    userdoll_pattern = \"\\$[^\\s]+|\\@[^\\s]+|\\&[^\\s]+|\\*[^\\s]+|[0-9][^\\s]+|\\~[^\\s]+\"\n",
    "    doc = re.sub(userdoll_pattern, '', doc)\n",
    "    \n",
    "    \n",
    "    # removing punctuation\n",
    "    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|\\?|_|,|=|;|>|<|\\.|\\@\"\n",
    "    doc = re.sub(punctuation, ' ', doc)\n",
    "    \n",
    "    return [w for w in doc.split() if len(w) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_k1 = k1_tweets_filtered[0]['text']\n",
    "tweet_k1_processed = pre_process(tweet_k1)\n",
    "\n",
    "print(tweet_k1)\n",
    "# tweet_k1_processed is now a list of words. \n",
    "# We use ' '.join() method to join the list to a string.\n",
    "print(' '.join(tweet_k1_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to display the first tweets stored in \n",
    "the variables k2_tweets_filtered and k3_tweets_filtered before and after they \n",
    "have been pre-processed using the function pre_process() supplied earlier.\n",
    "'''\n",
    "\n",
    "print((tweet := k2_tweets_filtered[0]['text']) + \"\\n\" + ' '.join(pre_process(tweet)) + \"\\n\")\n",
    "\n",
    "print((tweet := k3_tweets_filtered[0]['text']) + \"\\n\" + ' '.join(pre_process(tweet)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to pre-process and clean up all tweets \n",
    "stored in the variable k1_tweets_filtered, k2_tweets_filtered and k3_tweets_filtered using the \n",
    "function pre_process() to result in new variables k1_tweets_processed, k2_tweets_processed \n",
    "and k3_tweets_processed.\n",
    "'''\n",
    "\n",
    "k1_tweets_processed = [pre_process(tweet[\"text\"]) for tweet in k1_tweets_filtered]\n",
    "k2_tweets_processed = [pre_process(tweet[\"text\"]) for tweet in k2_tweets_filtered]\n",
    "k3_tweets_processed = [pre_process(tweet[\"text\"]) for tweet in k3_tweets_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to print out the first 5 processed tweets for each keyword.\n",
    "Hint: Each tweet in tweets_processed is now a list of words, not a string. \n",
    "      To print a string, you might need to use ' '.join(tweet), \n",
    "      when tweet is a processed tweet\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "print('The first 5 processed tweets for k1_tweets_processed:')\n",
    "\n",
    "for i in range(5):print(' '.join(k1_tweets_processed[i]))\n",
    "\n",
    "\n",
    "\n",
    "print('\\nThe first 5 processed tweets for k2_tweets_processed:')\n",
    "\n",
    "for i in range(5):print(' '.join(k2_tweets_processed[i]))\n",
    "\n",
    "\n",
    "print('\\nThe first 5 processed tweets for k3_tweets_processed:')\n",
    "\n",
    "for i in range(5):print(' '.join(k3_tweets_processed[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_termdoc(docs, vocab=[]):\n",
    "    \"\"\"\n",
    "    Construct a term-by-document-matrix\n",
    "    \n",
    "    docs: corpus\n",
    "    vocab: pre-defined vocabulary\n",
    "           if not supplied it will be automatically induced from the data\n",
    "    \n",
    "    returns the term-by-document matrix and the vocabulary of the passed corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # vocab is not passed\n",
    "    if vocab == []:\n",
    "        vocab = set()\n",
    "        termdoc_sparse = []\n",
    "\n",
    "        for doc in docs:       \n",
    "            # computes the frequencies of doc\n",
    "            doc_sparse = Counter(doc)    \n",
    "            termdoc_sparse.append(doc_sparse)\n",
    "            \n",
    "            # update the vocab\n",
    "            vocab.update(doc_sparse.keys())  \n",
    "\n",
    "        vocab = list(vocab)\n",
    "        vocab.sort()\n",
    "    \n",
    "    else:\n",
    "        termdoc_sparse = []        \n",
    "        for doc in docs:\n",
    "            termdoc_sparse.append(Counter(doc))\n",
    "            \n",
    "\n",
    "    n_docs = len(docs)\n",
    "    n_vocab = len(vocab)\n",
    "    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n",
    "\n",
    "    for j, doc_sparse in enumerate(termdoc_sparse):\n",
    "        for term, freq in doc_sparse.items():\n",
    "            try:\n",
    "                termdoc_dense[j, vocab.index(term)] = freq\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return termdoc_dense, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute the term-by-document matrix and the the dictionary from the collection of \n",
    "tweets collected for the first keyword\n",
    "'''\n",
    "\n",
    "k1_termdoc, k1_vocab = construct_termdoc(k1_tweets_processed)\n",
    "\n",
    "# print out the term-by-document matrix\n",
    "print(k1_termdoc)\n",
    "# print out the first 5 vocabulary entries\n",
    "print(' '.join(k1_vocab[0:5]))  # print out only the first 5 vocabulary entries\n",
    "\n",
    "# visualise the term-by-document matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(k1_termdoc)\n",
    "ax.set_xlabel('term (vocabulary)')\n",
    "ax.set_ylabel('documents (tweets)')\n",
    "ax.set_title('Term-by-Document matrix from tweets collected for keyword \\\"{}\\\"'.format(keywords[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(x,y):\n",
    "    '''\n",
    "    Compute and return the Euclidean distance between two vectors x and y\n",
    "    '''\n",
    "    \n",
    "    return np.sqrt(np.sum(np.power(np.subtract(x, y), 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x,y):\n",
    "    '''\n",
    "    Compute and return the cosine distance between two vectors x and y\n",
    "    '''\n",
    "\n",
    "    dot_product = np.dot(x, y)\n",
    "    x_magnitude = np.sqrt(np.sum(np.power(x, 2)))\n",
    "    y_magnitude = np.sqrt(np.sum(np.power(y, 2)))\n",
    "\n",
    "    return  1 - (dot_product / (x_magnitude * y_magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The function takes the termdoc matrix as the input and computes variables called \"euclidean_distance_matrix\" \n",
    "and \"cosine_distance_matrix\", which are matrices whose elements (i,j) store the Eulidean distance \n",
    "and the cosine distance between tweet i-th and i-jth.\n",
    "'''\n",
    "\n",
    "def compute_distance_matrices(termdoc):    \n",
    "    euclidean_distance_matrix = []\n",
    "    cosine_distance_matrix = []\n",
    "\n",
    "    for tweet_i in termdoc:\n",
    "        euclidean_distance_array = []\n",
    "        cosine_distance_array = []\n",
    "\n",
    "        for tweet_j in termdoc:\n",
    "            euclidean_distance_array.append(Euclidean_distance(tweet_i, tweet_j))\n",
    "            cosine_distance_array.append(cosine_distance(tweet_i, tweet_j))\n",
    "\n",
    "        euclidean_distance_matrix.append(np.array(euclidean_distance_array))\n",
    "        cosine_distance_matrix.append(np.array(cosine_distance_array))\n",
    "    \n",
    "    return np.array(euclidean_distance_matrix), np.array(cosine_distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distance matrices for k1_termdoc using the function \"compute_distance_matrices\"\n",
    "\n",
    "k1_euclidean_distance, k1_cosine_distance = compute_distance_matrices(k1_termdoc)\n",
    "\n",
    "# Visualise the distance matrices for this keyword\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Euclidean Distance Matrix\")\n",
    "plt.imshow(k1_euclidean_distance)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Cosine Distance Matrix\")\n",
    "plt.imshow(k1_cosine_distance)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(k1_euclidean_distance, k1_cosine_distance)\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Cosine Distance')\n",
    "plt.title('Relationship between Euclidean Distance and Cosine Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Compute the term-by-document matrix and the vocabulary for tweets stored \n",
    "in k2_tweets_processed\n",
    "'''\n",
    "\n",
    "k2_termdoc, k2_vocab = construct_termdoc(k2_tweets_processed)\n",
    "\n",
    "'''\n",
    "Code print out the first 5 vocabularies \n",
    "'''\n",
    "\n",
    "print(' '.join(k2_vocab[0:5]))\n",
    "\n",
    "'''\n",
    "code to visualise the term-by-document matrix\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(k2_termdoc)\n",
    "ax.set_xlabel('term (vocabulary)')\n",
    "ax.set_ylabel('documents (tweets)')\n",
    "ax.set_title('Term-by-Document matrix from tweets collected for keyword \\\"{}\\\"'.format(keywords[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2_euclidean_distance, k2_cosine_distance = compute_distance_matrices(k2_termdoc)\n",
    "\n",
    "# Visualise the distance matrix for this keyword\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Euclidean Distance Matrix\")\n",
    "plt.imshow(k2_euclidean_distance)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Cosine Distance Matrix\")\n",
    "plt.imshow(k2_cosine_distance)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Codes to compute the term-by-document matrix and the vocabulary for tweets stored \n",
    "in k3_tweets_processed\n",
    "'''\n",
    "\n",
    "\n",
    "k3_termdoc, k3_vocab = construct_termdoc(k3_tweets_processed)\n",
    "\n",
    "\n",
    "'''\n",
    "Code print out the first 5 vocabularies \n",
    "'''\n",
    "\n",
    "print(' '.join(k3_vocab[0:5]))\n",
    "\n",
    "\n",
    "'''\n",
    "Code to visualise the term-by-document matrix\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(k3_termdoc)\n",
    "ax.set_xlabel('term (vocabulary)')\n",
    "ax.set_ylabel('documents (tweets)')\n",
    "ax.set_title('Term-by-Document matrix from tweets collected for keyword \\\"{}\\\"'.format(keywords[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distance matrices for k1_termdoc using the function \"compute_distance_matrices\"\n",
    "\n",
    "k3_euclidean_distance, k3_cosine_distance = compute_distance_matrices(k3_termdoc)\n",
    "\n",
    "# Visualise the distance matrix for this keyword\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Euclidean Distance Matrix\")\n",
    "plt.imshow(k3_euclidean_distance)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Cosine Distance Matrix\")\n",
    "plt.imshow(k3_cosine_distance)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Produce a scatter plot of Euclidean vs cosine distance for all tweets.\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(all_euclidean_distances, all_cosine_distances, label=\"Default\")\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Cosine Distance')\n",
    "plt.title('Relationship between Euclidean Distance and Cosine Distance')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Fit first and second order polynomials to the data in the scatter plot and overplot it. \n",
    "\n",
    "first_order = [np.polyfit(all_euclidean_distances[index], all_cosine_distances[index], deg=1) for index in range(len(all_euclidean_distances))]\n",
    "second_order = [np.polyfit(all_euclidean_distances[index], all_cosine_distances[index], deg=2) for index in range(len(all_euclidean_distances))]\n",
    "first_order_x = [float(dd[0]) for dd in first_order]\n",
    "first_order_y = [float(dd[1]) for dd in first_order]\n",
    "\n",
    "plt.scatter(first_order_x, first_order_y, label=\"First Order\")  \n",
    "\n",
    "second_order_x = [float(dd[0]) for dd in second_order]\n",
    "second_order_y = [float(dd[1]) for dd in second_order]\n",
    "\n",
    "plt.scatter(second_order_x, second_order_y, label=\"Second Order\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initialise a kmeans object  from scikit-lean package\n",
    "'''\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=5, max_iter=3000,\n",
    "                verbose=True, tol=0.000001, random_state=123456)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perform clustering on the data stored in the variable all_termdoc\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "kmeans.fit(all_termdoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write your codes to print out the cluster centers.\n",
    "'''\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plot bar charts for each of the three clusters, obtained from KMeans, \n",
    "# where each bar chart has 20 strongest words sorted by their presence strength.\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "color_codes = ['deepskyblue', 'lime', 'darkorange']\n",
    "for i in range(3):\n",
    "    sort_index = np.argsort(-cluster_centers[i])\n",
    "    words = (np.array(all_vocab)[sort_index])[:20]\n",
    "    strength = (np.array(cluster_centers[i])[sort_index])[:20]\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.bar(words, strength, color = color_codes[i])\n",
    "    plt.title(\"Top 20 most used words in tweets of keyword {}.\".format(keywords[i]))\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Strength\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Codes to print out the first **200** cluster labels assigned to the first 200 tweets.\n",
    "'''\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(labels[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to obtain the labels of tweets for each keyword\n",
    "and store the labels of the first keyword in ***k1_labels***, \n",
    "the labels of the second keyword in ***k2_labels*** and\n",
    "the labels of the third keyword in ***k3_labels***.\n",
    "'''\n",
    "\n",
    "k1_labels = labels[:len(k1_termdoc)]\n",
    "k2_labels = labels[len(k1_termdoc):len(k1_termdoc) + len(k2_termdoc)]\n",
    "k3_labels = labels[len(k1_termdoc)+len(k2_termdoc):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to obtain the list of tweet indices of each keyword that are assigned to the first cluster.\n",
    "\n",
    "'''\n",
    "# obtain the list of tweet indices of keyword k1 that are assigned to the first cluster\n",
    "# means that to find tweet indices that have label 0 in k1_labels\n",
    "\n",
    "\n",
    "\n",
    "k1_idx_label0 = 0 \n",
    "for i in k1_labels:\n",
    "    if i == 0:\n",
    "        k1_idx_label0 += 1\n",
    "# obtain the list of tweet indices of keyword k2 that are assigned to the first cluster\n",
    "# means that to find tweet indices that have label 0 in k2_labels\n",
    "k2_idx_label0 = 0 \n",
    "for i in k2_labels:\n",
    "    if i == 0:\n",
    "        k2_idx_label0 += 1\n",
    "\n",
    "# obtain the list of tweet indices of keyword k3 that are assigned to the first cluster\n",
    "# means that to find tweet indices that have label 0 in k3_labels\n",
    "k3_idx_label0 = 0 \n",
    "for i in k3_labels:\n",
    "    if i == 0:\n",
    "        k3_idx_label0 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Plotted a bar chart to visualise the number of tweets of each keyword that are assigned to the first cluster.\n",
    "\n",
    "'''\n",
    "\n",
    "idx_label0 = [k1_idx_label0, k2_idx_label0, k3_idx_label0]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(keywords, idx_label0, width=0.5, color=color_codes)\n",
    "plt.title(\"Number of Tweets in cluster 0 from each Keyword\")\n",
    "plt.xlabel(\"Keyword\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the list of tweet indices of keyword k1 that are assigned to the second cluster\n",
    "# means that to find tweet indices that have label 1 in k1_labels\n",
    "\n",
    "\n",
    "\n",
    "k1_idx_label1 = 0 \n",
    "for i in k1_labels:\n",
    "    if i == 1:\n",
    "        k1_idx_label1 += 1\n",
    "\n",
    "# obtain the list of tweet indices of keyword k2 that are assigned to the second cluster\n",
    "# means that to find tweet indices that have label 1 in k2_labels\n",
    "k2_idx_label1 = 0 \n",
    "for i in k2_labels:\n",
    "    if i == 1:\n",
    "        k2_idx_label1 += 1\n",
    "\n",
    "\n",
    "# obtain the list of tweet indices of keyword k3 that are assigned to the second cluster\n",
    "# means that to find tweet indices that have label 1 in k3_labels\n",
    "k3_idx_label1 = 0 \n",
    "for i in k3_labels:\n",
    "    if i == 1:\n",
    "        k3_idx_label1 += 1\n",
    "\n",
    "# Plot a bar chart to visualise the number of tweets of each keyword that are assigned to the second cluster\n",
    "\n",
    "idx_label1 = [k1_idx_label1, k2_idx_label1, k3_idx_label1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(keywords, idx_label1, width=0.5, color=color_codes)\n",
    "plt.title(\"Number of Tweets in cluster 1 from each Keyword\")\n",
    "plt.xlabel(\"Keyword\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obtain the list of tweet indices of keyword k1 that are assigned to the third cluster\n",
    "# means that to find tweet indices that have label 2 in k1_labels\n",
    "\n",
    "\n",
    "k1_idx_label2 = 0 \n",
    "for i in k1_labels:\n",
    "    if i == 2:\n",
    "        k1_idx_label2 += 1\n",
    "\n",
    "# obtain the list of tweet indices of keyword k2 that are assigned to the third cluster\n",
    "# means that to find tweet indices that have label 2 in k2_labels\n",
    "k2_idx_label2 = 0 \n",
    "for i in k2_labels:\n",
    "    if i == 2:\n",
    "        k2_idx_label2 += 1\n",
    "\n",
    "# obtain the list of tweet indices of keyword k3 that are assigned to the third cluster\n",
    "# means that to find tweet indices that have label 2 in k3_labels\n",
    "k3_idx_label2 = 0 \n",
    "for i in k3_labels:\n",
    "    if i == 2:\n",
    "        k3_idx_label2 += 1\n",
    "\n",
    "# Plot a bar chart to visualise the number of tweets of each keyword that are assigned to the third cluster\n",
    "\n",
    "\n",
    "idx_label2 = [k1_idx_label2, k2_idx_label2, k3_idx_label2]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(keywords, idx_label2, width=0.5, color=color_codes)\n",
    "plt.title(\"Number of Tweets in cluster 2 from each Keyword\")\n",
    "plt.xlabel(\"Keyword\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
